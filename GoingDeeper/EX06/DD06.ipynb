{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8bfe876",
   "metadata": {},
   "source": [
    "## 번역 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d0cea9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "550548da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.21.4\n",
      "1.3.3\n",
      "2.6.0\n",
      "3.6.5\n",
      "4.1.2\n"
     ]
    }
   ],
   "source": [
    "import numpy \n",
    "import pandas \n",
    "import tensorflow \n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "print(numpy.__version__)\n",
    "print(pandas.__version__)\n",
    "print(tensorflow.__version__)\n",
    "print(nltk.__version__)\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f08ea6",
   "metadata": {},
   "source": [
    "챗봇 데이터 다운"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86a57665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/songys/Chatbot_data.git\n",
      " 212992/Unknown - 0s 1us/step슝=3\n"
     ]
    }
   ],
   "source": [
    "zip_path = tf.keras.utils.get_file(\n",
    "    'ChatbotData.csv',\n",
    "    origin='https://github.com/songys/Chatbot_data.git',\n",
    "    extract=True\n",
    ")\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f5e9ae",
   "metadata": {},
   "source": [
    "중복치 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7404d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/.keras/datasets/ChatbotData.csv\n"
     ]
    }
   ],
   "source": [
    "print(zip_path)  # zip_path가 정확한지 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8209e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: 11824\n",
      ">> 다 맞춰주고 싶은 사람,정말 사랑하고 있나봐요.,2\n",
      ">> 나만 친구로 생각했나봐,그런 친구는 거르세요.,0\n",
      ">> 짝녀가 나를 떠보는 걸까.,입장을 확실히 하는 편이 좋겠네요.,2\n",
      ">> 기 빨렸어,너무 긴장했나봐요.,0\n",
      ">> 마음도 춥고 날씨도 춥네,마음 감기조심하세요.,0\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./data/ChatbotData.csv\"\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "    spa_eng_sentences = f.read().splitlines()\n",
    "\n",
    "spa_eng_sentences = list(set(spa_eng_sentences)) \n",
    "total_sentence_count = len(spa_eng_sentences)\n",
    "print(\"Example:\", total_sentence_count)\n",
    "\n",
    "for sen in spa_eng_sentences[0:100][::20]: \n",
    "    print(\">>\", sen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "512b9567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Size:  59\n",
      "\n",
      "\n",
      "Train Questions Example: 11765\n",
      ">> Q\n",
      ">> 가스비 너무 많이 나왔다.\n",
      ">> 가출해도 갈 데가 없어\n",
      ">> 감정이 쓰레기통처럼 엉망진창이야\n",
      ">> 같이 할 수 있는 취미 생활 뭐 있을까\n",
      "\n",
      "Train Answers Example: 11765\n",
      ">> A\n",
      ">> 다음 달에는 더 절약해봐요.\n",
      ">> 선생님이나 기관에 연락해보세요.\n",
      ">> 자신을 더 사랑해주세요.\n",
      ">> 함께하면 서로를 더 많이 알게 될 거예요.\n",
      "\n",
      "\n",
      "Test Questions Example: 59\n",
      ">> 학교샘 좋아하는 사람 있나?\n",
      ">> 헤어지고 나서 알았어 사랑했다는 걸\n",
      ">> 혼자가 편하다는 짝남에게 먼저 대쉬해버림.\n",
      "\n",
      "Test Answers Example: 59\n",
      ">> 존경의 의미라고 생각하세요.\n",
      ">> 진정한 사랑이라고 생각한다면 다시 연락해보세요.\n",
      ">> 거절의 뜻은 아니었나요.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV 파일 경로\n",
    "file_path = \"./data/ChatbotData.csv\"\n",
    "\n",
    "# pandas로 CSV 파일 읽기\n",
    "data = pd.read_csv(file_path, header=None)\n",
    "\n",
    "# 질문과 답변을 나눠서 저장\n",
    "questions = data[0]  # 첫 번째 열이 질문\n",
    "answers = data[1]    # 두 번째 열이 답변\n",
    "\n",
    "# 전체 데이터 개수\n",
    "total_sentence_count = len(questions)\n",
    "\n",
    "# 테스트 데이터 개수 (전체 데이터의 1/200)\n",
    "test_sentence_count = total_sentence_count // 200\n",
    "print(\"Test Size: \", test_sentence_count)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Train/Test 데이터 나누기\n",
    "train_questions = questions[:-test_sentence_count]\n",
    "test_questions = questions[-test_sentence_count:]\n",
    "train_answers = answers[:-test_sentence_count]\n",
    "test_answers = answers[-test_sentence_count:]\n",
    "\n",
    "# Train 데이터 확인\n",
    "print(\"Train Questions Example:\", len(train_questions))\n",
    "for sen in train_questions[0:100][::20]:\n",
    "    print(\">>\", sen)\n",
    "print(\"\\nTrain Answers Example:\", len(train_answers))\n",
    "for sen in train_answers[0:100][::20]:\n",
    "    print(\">>\", sen)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Test 데이터 확인\n",
    "print(\"Test Questions Example:\", len(test_questions))\n",
    "for sen in test_questions[0:100][::20]:\n",
    "    print(\">>\", sen)\n",
    "print(\"\\nTest Answers Example:\", len(test_answers))\n",
    "for sen in test_answers[0:100][::20]:\n",
    "    print(\">>\", sen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3419cd3",
   "metadata": {},
   "source": [
    "전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "427dbf35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요! this is a test 문장입니다. numbers 1234, 특수기호\n"
     ]
    }
   ],
   "source": [
    "# Q. 전처리 함수를 만들어 보세요. 아래 기능을 추가해주세요.\n",
    "import re\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower()  # 모든 문자를 소문자로 변환\n",
    "    sentence = re.sub(r'[^a-zA-Z0-9가-힣.,!? ]+', '', sentence)  # 영문자, 숫자, 한글, 주요 특수문자를 제외하고 제거\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)  # 둘 이상의 공백을 하나로 치환\n",
    "    sentence = sentence.strip()  # 문자열 양 끝 공백 제거\n",
    "    return sentence\n",
    "\n",
    "# 테스트\n",
    "test_sentence = \"안녕하세요! This is a TEST 문장입니다. Numbers: 1234, 특수기호: #@$%^&*()\"\n",
    "processed_sentence = preprocess_sentence(test_sentence)\n",
    "print(processed_sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4ca11ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "spa_eng_sentences = list(map(preprocess_sentence, spa_eng_sentences))\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b837184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Size:  59\n",
      "\n",
      "\n",
      "Train Questions Example: 11765\n",
      ">> Q\n",
      ">> 가스비 너무 많이 나왔다.\n",
      ">> 가출해도 갈 데가 없어\n",
      ">> 감정이 쓰레기통처럼 엉망진창이야\n",
      ">> 같이 할 수 있는 취미 생활 뭐 있을까\n",
      "\n",
      "\n",
      "Train Answers Example: 11765\n",
      ">> A\n",
      ">> 다음 달에는 더 절약해봐요.\n",
      ">> 선생님이나 기관에 연락해보세요.\n",
      ">> 자신을 더 사랑해주세요.\n",
      ">> 함께하면 서로를 더 많이 알게 될 거예요.\n",
      "\n",
      "\n",
      "Test Questions Example: 59\n",
      ">> 학교샘 좋아하는 사람 있나?\n",
      ">> 헤어지고 나서 알았어 사랑했다는 걸\n",
      ">> 혼자가 편하다는 짝남에게 먼저 대쉬해버림.\n",
      "\n",
      "\n",
      "Test Answers Example: 59\n",
      ">> 존경의 의미라고 생각하세요.\n",
      ">> 진정한 사랑이라고 생각한다면 다시 연락해보세요.\n",
      ">> 거절의 뜻은 아니었나요.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 전체 질문과 답변 개수 계산\n",
    "total_sentence_count = len(questions)\n",
    "test_sentence_count = total_sentence_count // 200\n",
    "print(\"Test Size: \", test_sentence_count)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Train/Test 데이터 나누기 - 질문\n",
    "train_questions = questions[:-test_sentence_count]\n",
    "test_questions = questions[-test_sentence_count:]\n",
    "\n",
    "# Train/Test 데이터 나누기 - 답변\n",
    "train_answers = answers[:-test_sentence_count]\n",
    "test_answers = answers[-test_sentence_count:]\n",
    "\n",
    "# Train 데이터 확인 (질문과 답변)\n",
    "print(\"Train Questions Example:\", len(train_questions))\n",
    "for sen in train_questions[0:100][::20]: \n",
    "    print(\">>\", sen)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Train Answers Example:\", len(train_answers))\n",
    "for sen in train_answers[0:100][::20]: \n",
    "    print(\">>\", sen)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Test 데이터 확인 (질문과 답변)\n",
    "print(\"Test Questions Example:\", len(test_questions))\n",
    "for sen in test_questions[0:100][::20]: \n",
    "    print(\">>\", sen)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Test Answers Example:\", len(test_answers))\n",
    "for sen in test_answers[0:100][::20]: \n",
    "    print(\">>\", sen)\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0baf0894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def split_spa_eng_sentences(spa_eng_sentences):\n",
    "    spa_sentences = []\n",
    "    eng_sentences = []\n",
    "    for spa_eng_sentence in tqdm(spa_eng_sentences):\n",
    "        eng_sentence, spa_sentence = spa_eng_sentence.split('\\t')\n",
    "        spa_sentences.append(spa_sentence)\n",
    "        eng_sentences.append(eng_sentence)\n",
    "    return eng_sentences, spa_sentences\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88b060a",
   "metadata": {},
   "source": [
    "토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89887010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def generate_tokenizer(corpus,\n",
    "                       vocab_size,\n",
    "                       lang=\"spa-eng\",\n",
    "                       pad_id=0,   # pad token의 일련번호\n",
    "                       bos_id=1,  # 문장의 시작을 의미하는 bos token(<s>)의 일련번호\n",
    "                       eos_id=2,  # 문장의 끝을 의미하는 eos token(</s>)의 일련번호\n",
    "                       unk_id=3):   # unk token의 일련번호\n",
    "    file = \"./%s_corpus.txt\" % lang\n",
    "    model = \"%s_spm\" % lang\n",
    "\n",
    "    with open(file, 'w') as f:\n",
    "        for row in corpus: f.write(str(row) + '\\n')\n",
    "\n",
    "    import sentencepiece as spm\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        '--input=./%s --model_prefix=%s --vocab_size=%d'\\\n",
    "        % (file, model, vocab_size) + \\\n",
    "        '--pad_id==%d --bos_id=%d --eos_id=%d --unk_id=%d'\\\n",
    "        % (pad_id, bos_id, eos_id, unk_id)\n",
    "    )\n",
    "\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load('%s.model' % model)\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ce6555",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06531e6a",
   "metadata": {},
   "source": [
    "## 동의어 치환 증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "23d9beec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# 동의어 딕셔너리 예시\n",
    "synonym_dict = {\n",
    "    \"좋다\": [\"훌륭하다\", \"멋지다\", \"아름답다\", \"재미있다\", \"신나다\"],\n",
    "    \"나쁘다\": [\"형편없다\", \"끔찍하다\", \"불쾌하다\"],\n",
    "    \"어렵다\": [\"힘들다\", \"어렵다\", \"복잡하다\"],\n",
    "    \"쉽다\": [\"간단하다\", \"수월하다\"],\n",
    "    \"사랑하다\": [\"좋아하다\", \"애정하다\", \"선호하다\"],\n",
    "    # 추가 단어와 동의어 정의\n",
    "}\n",
    "\n",
    "def synonym_replacement(sentence):\n",
    "    words = sentence.split()\n",
    "    new_words = words.copy()\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        if word in synonym_dict:\n",
    "            synonym = random.choice(synonym_dict[word])\n",
    "            new_words[i] = synonym\n",
    "\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "# 예시 문장 증강\n",
    "augmented_questions = [synonym_replacement(q) for q in train_questions]\n",
    "augmented_answers = [synonym_replacement(a) for a in train_answers]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "938a2d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=././questions-answers_corpus.txt --model_prefix=questions-answers_spm --vocab_size=11692--pad_id==0 --bos_id=1 --eos_id=2 --unk_id=3\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ././questions-answers_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: questions-answers_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 11692\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  re"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "move_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: ././questions-answers_corpus.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 11765 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=666984\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9505% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1090\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999505\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 11765 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 31140 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 11765\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 42641\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 42641 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=16533 obj=16.4892 num_tokens=125169 num_tokens/piece=7.57086\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=15044 obj=15.227 num_tokens=125486 num_tokens/piece=8.34127\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=12860 obj=15.3513 num_tokens=127670 num_tokens/piece=9.92768\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=12856 obj=15.3135 num_tokens=127712 num_tokens/piece=9.93404\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: questions-answers_spm.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: questions-answers_spm.vocab\n"
     ]
    }
   ],
   "source": [
    "# 원래 데이터와 증강 데이터를 합침\n",
    "all_questions = train_questions + augmented_questions\n",
    "all_answers = train_answers + augmented_answers\n",
    "\n",
    "# vocab_size를 적절히 조정 (예: 10000)\n",
    "VOCAB_SIZE = min(30000, len(set(all_questions + all_answers)))\n",
    "\n",
    "# 새로운 토크나이저 생성\n",
    "tokenizer = generate_tokenizer(all_questions + all_answers, VOCAB_SIZE, 'questions-answers')\n",
    "tokenizer.set_encode_extra_options(\"bos:eos\")  # 문장 양 끝에 <s>, </s> 추가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ac954809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=././questions-answers_corpus.txt --model_prefix=questions-answers_spm --vocab_size=10000--pad_id==0 --bos_id=1 --eos_id=2 --unk_id=3\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ././questions-answers_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: questions-answers_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 10000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: ././questions-answers_corpus.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 11765 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=339361\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9502% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1087\n",
      "trainer_interface.cc(488) LOG(INFO) Final character covera"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ge=0.999502\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 11765 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 21294 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 11765\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 25859\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 25859 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=12509 obj=15.3532 num_tokens=66941 num_tokens/piece=5.35143\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=11501 obj=14.2166 num_tokens=67163 num_tokens/piece=5.83975\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=10995 obj=14.2136 num_tokens=67416 num_tokens/piece=6.13151\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=10944 obj=14.1978 num_tokens=67480 num_tokens/piece=6.16594\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: questions-answers_spm.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: questions-answers_spm.vocab\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 10000  # 가능한 단어 수에 맞춰 설정\n",
    "tokenizer = generate_tokenizer(train_questions + train_answers, VOCAB_SIZE, 'questions-answers')\n",
    "tokenizer.set_encode_extra_options(\"bos:eos\")  # 문장 양 끝에 <s>, </s> 추가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c7f157b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def make_corpus(sentences, tokenizer):\n",
    "    corpus = []\n",
    "    for sentence in tqdm(sentences):\n",
    "        tokens = tokenizer.encode_as_ids(sentence)\n",
    "        corpus.append(tokens)\n",
    "    return corpus\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36e854ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6efff589a5b4e5c91ff44a896391eaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11765 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3120b129184cb0a6e0f6722f5c8d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11765 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "questions_corpus = make_corpus(train_questions, tokenizer)  # 질문 데이터로 코퍼스 생성\n",
    "answers_corpus = make_corpus(train_answers, tokenizer)      # 답변 데이터로 코퍼스 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3cb83bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      "[1, 4, 3, 2]\n",
      "\n",
      "\n",
      "A\n",
      "[1, 4901, 2]\n"
     ]
    }
   ],
   "source": [
    "print(train_questions[0])  # 첫 번째 질문 출력\n",
    "print(questions_corpus[0])  # 첫 번째 질문 코퍼스 출력\n",
    "print('\\n')\n",
    "print(train_answers[0])  # 첫 번째 답변 출력\n",
    "print(answers_corpus[0])  # 첫 번째 답변 코퍼스 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad0d49bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 50\n",
    "questions_ndarray = tf.keras.preprocessing.sequence.pad_sequences(questions_corpus, maxlen=MAX_LEN, padding='post')\n",
    "answers_ndarray = tf.keras.preprocessing.sequence.pad_sequences(answers_corpus, maxlen=MAX_LEN, padding='post')\n",
    "\n",
    "print('슝=3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e559aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((questions_ndarray, answers_ndarray)).batch(batch_size=BATCH_SIZE)\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b41bdea",
   "metadata": {},
   "source": [
    "## 번역 모델 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1db88ec",
   "metadata": {},
   "source": [
    "positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "78d5e720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Positional Encoding 구현\n",
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, (2*(i//2)) / np.float32(d_model))\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table\n",
    "\n",
    "    return sinusoid_table\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "afe0553e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_lookahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_enc_mask = generate_padding_mask(src)\n",
    "\n",
    "    dec_lookahead_mask = generate_lookahead_mask(tgt.shape[1])\n",
    "    dec_tgt_padding_mask = generate_padding_mask(tgt)\n",
    "    dec_mask = tf.maximum(dec_tgt_padding_mask, dec_lookahead_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3b57967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "        \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "    \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "                        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "            \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cce50dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "90c623c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4c10e23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        '''\n",
    "        Masked Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        # Q, K, V 순서에 주의하세요!\n",
    "        out, dec_enc_attn = self.enc_dec_attn(Q=out, K=enc_out, V=enc_out, mask=dec_enc_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fa542dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "    \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ab439cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, dec_enc_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "783ac66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, dec_enc_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, dec_enc_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d739e43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\t\t\n",
    "d_model = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1a25bea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "765f4b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                        beta_1=0.9,\n",
    "                                        beta_2=0.98, \n",
    "                                        epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d4ed347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "158b3239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(questions_batch, answers_batch, optimizer):\n",
    "    enc_padding_mask, dec_padding_mask, look_ahead_mask = create_masks(questions_batch, answers_batch)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = transformer(questions_batch, answers_batch, \n",
    "                                  enc_padding_mask, \n",
    "                                  dec_padding_mask, \n",
    "                                  look_ahead_mask, \n",
    "                                  training=True)\n",
    "\n",
    "        # predictions가 tuple인 경우 첫 번째 요소를 선택\n",
    "        if isinstance(predictions, tuple):\n",
    "            predictions = predictions[0]  # 필요한 요소로 변경\n",
    "        \n",
    "        loss = loss_fn(answers_batch, predictions)\n",
    "\n",
    "    # 가중치 업데이트\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5ff8386b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13cbaf2def5443d591f6dc00ad5be719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d7b05c87bf64e99bf7366b622a08fb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb8c7fc031c4727be0eb5bcdb90005b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed for 3 epochs. Final loss: 0.5548\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    dataset_count = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "    tqdm_bar = tqdm(total=dataset_count)\n",
    "\n",
    "    # train_dataset에서 배치를 가져오도록 수정\n",
    "    for questions_batch, answers_batch in train_dataset:\n",
    "        batch_loss = train_step(questions_batch, answers_batch, optimizer)\n",
    "        total_loss += batch_loss.numpy()\n",
    "\n",
    "        \n",
    "        # tqdm 진행 표시줄 업데이트\n",
    "        tqdm_bar.update(1)  # 한 배치 처리 후 업데이트\n",
    "        tqdm_bar.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        tqdm_bar.set_postfix_str('Loss %.4f' % (total_loss / (tqdm_bar.n + 1)))\n",
    "\n",
    "    tqdm_bar.close()  # tqdm 종료\n",
    "\n",
    "# 모델 훈련 후 결과 출력\n",
    "print(f\"Training completed for {EPOCHS} epochs. Final loss: {total_loss / dataset_count:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2261eb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 모델 훈련 코드 (기존 코드)\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    dataset_count = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "    tqdm_bar = tqdm(total=dataset_count)\n",
    "\n",
    "    for questions_batch, answers_batch in train_dataset:\n",
    "        batch_loss = train_step(questions_batch, answers_batch, optimizer)\n",
    "        total_loss += batch_loss.numpy()\n",
    "        \n",
    "        # tqdm 진행 표시줄 업데이트\n",
    "        tqdm_bar.update(1)\n",
    "        tqdm_bar.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        tqdm_bar.set_postfix_str('Loss %.4f' % (total_loss / (tqdm_bar.n + 1)))\n",
    "\n",
    "    tqdm_bar.close()\n",
    "\n",
    "# 모델 훈련 후 결과 출력\n",
    "print(f\"Training completed for {EPOCHS} epochs. Final loss: {total_loss / dataset_count:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c322fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
