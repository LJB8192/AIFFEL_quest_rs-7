{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b550ad0b",
   "metadata": {},
   "source": [
    "# 10. 트랜스포머로 만드는 대화형 챗봇\n",
    "\n",
    "- 평가기준\n",
    "1. 한국어 전처리를 통해 학습 데이터셋 구축\n",
    ": 공백, 특수문자처리, 토크나이징, 병렬 데이터 구축\n",
    "2. 트랜스포머 모델을 구현하여 한국어 챗봇 모델 학습 정상적 진행\n",
    ": 구현한 트랜스포머 모델이 한국어 병렬 데이터 학습 시 안정적 수렴\n",
    "3. 한국어 입력 문장에 대해 한국어로 대답하는 함수 구현\n",
    ": 한국어 입력 문장 맥락에 맞는 한국어로 답변 리턴"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1af7b8",
   "metadata": {},
   "source": [
    "#### 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55eec365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7febbd7b",
   "metadata": {},
   "source": [
    "## Step 1. 데이터 수집하기\n",
    "\n",
    "- cloud shell을 통한 다운로드를 진행함. \n",
    "- 데이터 출처 : https://github.com/songys/Chatbot_data/blob/master/ChatbotData.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dbb50c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 파일을 다운로드하고 압축 해제\n",
    "path_to_csv = tf.keras.utils.get_file(\n",
    "    'ChatbotData.csv', \n",
    "    origin='https://github.com/songys/Chatbot_data/raw/master/ChatbotData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e336af8",
   "metadata": {},
   "source": [
    "# Step 2. 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81391140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "# 사용할 샘플의 최대 개수\n",
    "MAX_SAMPLES = 50000\n",
    "print(MAX_SAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92ce939",
   "metadata": {},
   "source": [
    "### 한글 전처리를 위한 함수 수정\n",
    "- 영문 전처리에서 사용하던 소문자화 부분을 제외함. 공백 제거만 사용.\n",
    "- 구두점 사이 거리를 만드는 코드 그대로 사용\n",
    "- 구두점 제외 문자열 추출 과정에서 한국어 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55f1b9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수\n",
    "def preprocess_sentence(sentence):\n",
    "    # 공백 제거\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    # 구두점과 단어 사이의 거리를 만듦\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    \n",
    "    # 구두점 제외 문자열 추출 (한국어, 영어, 숫자만 남김)\n",
    "    sentence = re.sub(\"[^가-힣a-zA-Z0-9?.!,]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    # 연속된 ㅋ, ㅎ, ㅠ 정규화 - 승환님 참고 \n",
    "    sentence = re.sub(r'ㅋ{2,}', 'ㅋㅋ', sentence)\n",
    "    sentence = re.sub(r'ㅎ{2,}', 'ㅎㅎ', sentence)\n",
    "    sentence = re.sub(r'ㅠ{2,}', 'ㅠㅠ', sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3501f601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문과 답변 데이터를 로드하고 전처리하는 함수\n",
    "def load_conversations():\n",
    "    inputs, outputs = [], []\n",
    "    \n",
    "    # CSV 파일 열기\n",
    "    with open(path_to_csv, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # 첫 번째 줄(헤더)을 건너뜀\n",
    "        \n",
    "        for row in reader:\n",
    "            # 각 행에서 질문과 답변을 추출\n",
    "            question, answer = row[0], row[1]\n",
    "            \n",
    "            # 전처리 함수를 질문과 답변에 적용\n",
    "            inputs.append(preprocess_sentence(question))\n",
    "            outputs.append(preprocess_sentence(answer))\n",
    "            \n",
    "            # 최대 샘플 수만큼 데이터를 가져옴\n",
    "            if len(inputs) >= MAX_SAMPLES:\n",
    "                break\n",
    "\n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69728330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 수 : 11823\n",
      "전체 샘플 수 : 11823\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 로드하고 전처리하여 질문을 questions, 답변을 answers에 저장\n",
    "questions, answers = load_conversations()\n",
    "print('전체 샘플 수 :', len(questions))\n",
    "print('전체 샘플 수 :', len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "099b4b6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 후의 22번째 질문 샘플: 가스비 장난 아님\n",
      "전처리 후의 22번째 답변 샘플: 다음 달에는 더 절약해봐요 .\n"
     ]
    }
   ],
   "source": [
    "print('전처리 후의 22번째 질문 샘플: {}'.format(questions[21]))\n",
    "print('전처리 후의 22번째 답변 샘플: {}'.format(answers[21]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2215c1e",
   "metadata": {},
   "source": [
    "## Step3. SubwordTextEncoder 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77820784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "살짝 오래 걸릴 수 있어요. 스트레칭 한 번 해볼까요? 👐\n",
      "슝=3 \n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "print(\"살짝 오래 걸릴 수 있어요. 스트레칭 한 번 해볼까요? 👐\")\n",
    "\n",
    "# 질문과 답변 데이터셋에 대해서 Vocabulary 생성\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)\n",
    "print(\"슝=3 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e23a420a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰에 고유한 정수를 부여합니다.\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed0cda10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_TOKEN의 번호 : [8162]\n",
      "END_TOKEN의 번호 : [8163]\n"
     ]
    }
   ],
   "source": [
    "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "769a224c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8164\n"
     ]
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰을 고려하여 +2를 하여 단어장의 크기를 산정합니다.\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "793f7814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 21번째 질문 샘플: [5757, 610, 2486, 4158]\n",
      "정수 인코딩 후의 21번째 답변 샘플: [2355, 7502, 7, 6266, 97, 1]\n"
     ]
    }
   ],
   "source": [
    "# 임의의 22번째 샘플에 대해서 정수 인코딩 작업을 수행.\n",
    "# 각 토큰을 고유한 정수로 변환\n",
    "print('정수 인코딩 후의 21번째 질문 샘플: {}'.format(tokenizer.encode(questions[21])))\n",
    "print('정수 인코딩 후의 21번째 답변 샘플: {}'.format(tokenizer.encode(answers[21])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13e67097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "# 샘플의 최대 허용 길이 또는 패딩 후의 최종 길이\n",
    "MAX_LENGTH = 50\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57a5d3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "  \n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    # 최대 길이 40 이하인 경우에만 데이터셋으로 허용\n",
    "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "      tokenized_inputs.append(sentence1)\n",
    "      tokenized_outputs.append(sentence2)\n",
    "  \n",
    "  # 최대 길이 40으로 모든 데이터셋을 패딩\n",
    "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  \n",
    "  return tokenized_inputs, tokenized_outputs\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8bf5cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 8164\n",
      "필터링 후의 질문 샘플 개수: 11823\n",
      "필터링 후의 답변 샘플 개수: 11823\n"
     ]
    }
   ],
   "source": [
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "print('단어장의 크기 :',(VOCAB_SIZE))\n",
    "print('필터링 후의 질문 샘플 개수: {}'.format(len(questions)))\n",
    "print('필터링 후의 답변 샘플 개수: {}'.format(len(answers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd96b7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
    "# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baba7c8f",
   "metadata": {},
   "source": [
    "## Step 4. 모델 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "263b58dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "  # 인코더에서 패딩을 위한 마스크\n",
    "  enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "\n",
    "  # 디코더에서 미래의 토큰을 마스크 하기 위해서 사용합니다.\n",
    "  # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
    "  look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask,\n",
    "      output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "  # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
    "  # 디코더에서 패딩을 위한 마스크\n",
    "  dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "  # 인코더\n",
    "  enc_outputs = encoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "  # 디코더\n",
    "  dec_outputs = decoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "  # 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6efebba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 포지셔널 인코딩 레이어\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, position, d_model):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "  def get_angles(self, position, i, d_model):\n",
    "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "    return position * angles\n",
    "\n",
    "  def positional_encoding(self, position, d_model):\n",
    "    # 각도 배열 생성\n",
    "    angle_rads = self.get_angles(\n",
    "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "        d_model=d_model)\n",
    "\n",
    "    # 배열의 짝수 인덱스에는 sin 함수 적용\n",
    "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "    # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
    "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    # sin과 cosine이 교차되도록 재배열\n",
    "    pos_encoding = tf.stack([sines, cosines], axis=0)\n",
    "    pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n",
    "    pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
    "\n",
    "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650b3449",
   "metadata": {},
   "source": [
    "- scaled dot product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b73bec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 스케일드 닷 프로덕트 어텐션 함수\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
    "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # 가중치를 정규화\n",
    "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "  logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # 패딩에 마스크 추가\n",
    "  if mask is not None:\n",
    "    logits += (mask * -1e9)\n",
    "\n",
    "  # softmax적용\n",
    "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "  # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
    "  output = tf.matmul(attention_weights, value)\n",
    "  return output\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f53039",
   "metadata": {},
   "source": [
    "- multihead attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1427a99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "    super(MultiHeadAttention, self).__init__(name=name)\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "  def split_heads(self, inputs, batch_size):\n",
    "    inputs = tf.reshape(\n",
    "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, inputs):\n",
    "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "    batch_size = tf.shape(query)[0]\n",
    "\n",
    "    # Q, K, V에 각각 Dense를 적용합니다\n",
    "    query = self.query_dense(query)\n",
    "    key = self.key_dense(key)\n",
    "    value = self.value_dense(value)\n",
    "\n",
    "    # 병렬 연산을 위한 머리를 여러 개 만듭니다\n",
    "    query = self.split_heads(query, batch_size)\n",
    "    key = self.split_heads(key, batch_size)\n",
    "    value = self.split_heads(value, batch_size)\n",
    "\n",
    "    # 스케일드 닷 프로덕트 어텐션 함수\n",
    "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "    # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "\n",
    "    # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
    "    outputs = self.dense(concat_attention)\n",
    "\n",
    "    return outputs\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d23667",
   "metadata": {},
   "source": [
    "- look ahead mask 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d440f6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def create_padding_mask(x):\n",
    "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "  # (batch_size, 1, 1, sequence length)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03e34f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "  seq_len = tf.shape(x)[1]\n",
    "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "  padding_mask = create_padding_mask(x)\n",
    "  return tf.maximum(look_ahead_mask, padding_mask)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07477549",
   "metadata": {},
   "source": [
    "- encoder 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da6417e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 인코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "  # 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "  attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "  attention = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "  # 두 번째 서브 레이어 : 2개의 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4c6be62",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "  # 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 임베딩 레이어\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # 포지셔널 인코딩\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # num_layers만큼 쌓아올린 인코더의 층.\n",
    "  for i in range(num_layers):\n",
    "    outputs = encoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name=\"encoder_layer_{}\".format(i),\n",
    "    )([outputs, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d458e5d",
   "metadata": {},
   "source": [
    "- decoder 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a4d88f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 디코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "  attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "      })\n",
    "\n",
    "  # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention1 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "  # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
    "  attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1,\n",
    "          'key': enc_outputs,\n",
    "          'value': enc_outputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # 마스크드 멀티 헤드 어텐션의 결과는\n",
    "  # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "  attention2 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "  # 세 번째 서브 레이어 : 2개의 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b09e116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "  # 패딩 마스크\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "  \n",
    "  # 임베딩 레이어\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # 포지셔널 인코딩\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  for i in range(num_layers):\n",
    "    outputs = decoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name='decoder_layer_{}'.format(i),\n",
    "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2410284",
   "metadata": {},
   "source": [
    "### 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7cd66627",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 256)    4198400     inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 256)    5253120     dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8164)   2098148     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 11,549,668\n",
      "Trainable params: 11,549,668\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 4 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 256 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.01 # 드롭아웃의 비율\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6b13085",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=4,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model2 = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=8,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model3 = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=16,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cff2930",
   "metadata": {},
   "source": [
    "### 손실 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "416ac550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  \n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "  loss = tf.multiply(loss, mask)\n",
    "\n",
    "  return tf.reduce_mean(loss)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e250f2c3",
   "metadata": {},
   "source": [
    "### 커스텀된 학습률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a4600ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps**-1.6)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75516e23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_learning_rate = CustomSchedule(d_model=128)\n",
    "\n",
    "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d515f56a",
   "metadata": {},
   "source": [
    "### 모델 컴파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e382902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model1.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3d541c",
   "metadata": {},
   "source": [
    "### 훈련하기\n",
    "- 정확도가 낮게 나오는 이유가 model 복잡도가 낮기 때문이라고 생각했음\n",
    "\n",
    "→ encoder, decoder 층의 깊이를 제어하는 num layer를 4, 8, 16으로 늘려 3개의 모델을 테스트 해봄.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c7d50a7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "93/93 [==============================] - 28s 173ms/step - loss: 1.2526 - accuracy: 1.8988e-04\n",
      "Epoch 2/50\n",
      "93/93 [==============================] - 16s 175ms/step - loss: 1.2022 - accuracy: 0.0189\n",
      "Epoch 3/50\n",
      "93/93 [==============================] - 16s 176ms/step - loss: 1.1543 - accuracy: 0.0221\n",
      "Epoch 4/50\n",
      "93/93 [==============================] - 17s 178ms/step - loss: 1.1260 - accuracy: 0.0211\n",
      "Epoch 5/50\n",
      "93/93 [==============================] - 17s 179ms/step - loss: 1.0991 - accuracy: 0.0351\n",
      "Epoch 6/50\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 1.0603 - accuracy: 0.0393\n",
      "Epoch 7/50\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 1.0284 - accuracy: 0.0393\n",
      "Epoch 8/50\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 0.9981 - accuracy: 0.0394\n",
      "Epoch 9/50\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 0.9664 - accuracy: 0.0396\n",
      "Epoch 10/50\n",
      "93/93 [==============================] - 17s 177ms/step - loss: 0.9344 - accuracy: 0.0397\n",
      "Epoch 11/50\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 0.9030 - accuracy: 0.0397\n",
      "Epoch 12/50\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 0.8733 - accuracy: 0.0398\n",
      "Epoch 13/50\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 0.8457 - accuracy: 0.0398\n",
      "Epoch 14/50\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 0.8202 - accuracy: 0.0400\n",
      "Epoch 15/50\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 0.7969 - accuracy: 0.0406\n",
      "Epoch 16/50\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 0.7762 - accuracy: 0.0415\n",
      "Epoch 17/50\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 0.7584 - accuracy: 0.0424\n",
      "Epoch 18/50\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 0.7428 - accuracy: 0.0432\n",
      "Epoch 19/50\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 0.7289 - accuracy: 0.0439\n",
      "Epoch 20/50\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 0.7159 - accuracy: 0.0446\n",
      "Epoch 21/50\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 0.7030 - accuracy: 0.0454\n",
      "Epoch 22/50\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 0.6906 - accuracy: 0.0462\n",
      "Epoch 23/50\n",
      "93/93 [==============================] - 17s 177ms/step - loss: 0.6786 - accuracy: 0.0470\n",
      "Epoch 24/50\n",
      "93/93 [==============================] - 17s 177ms/step - loss: 0.6668 - accuracy: 0.0477\n",
      "Epoch 25/50\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 0.6551 - accuracy: 0.0487\n",
      "Epoch 26/50\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 0.6429 - accuracy: 0.0495\n",
      "Epoch 27/50\n",
      "93/93 [==============================] - 17s 178ms/step - loss: 0.6309 - accuracy: 0.0507\n",
      "Epoch 28/50\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 0.6182 - accuracy: 0.0519\n",
      "Epoch 29/50\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 0.6058 - accuracy: 0.0530\n",
      "Epoch 30/50\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 0.5930 - accuracy: 0.0543\n",
      "Epoch 31/50\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 0.5793 - accuracy: 0.0558\n",
      "Epoch 32/50\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 0.5660 - accuracy: 0.0573\n",
      "Epoch 33/50\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 0.5514 - accuracy: 0.0592\n",
      "Epoch 34/50\n",
      "93/93 [==============================] - 17s 177ms/step - loss: 0.5372 - accuracy: 0.0610\n",
      "Epoch 35/50\n",
      "93/93 [==============================] - 17s 178ms/step - loss: 0.5229 - accuracy: 0.0631\n",
      "Epoch 36/50\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 0.5077 - accuracy: 0.0651\n",
      "Epoch 37/50\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 0.4926 - accuracy: 0.0672\n",
      "Epoch 38/50\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 0.4775 - accuracy: 0.0692\n",
      "Epoch 39/50\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 0.4617 - accuracy: 0.0714\n",
      "Epoch 40/50\n",
      "93/93 [==============================] - 17s 177ms/step - loss: 0.4459 - accuracy: 0.0738\n",
      "Epoch 41/50\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 0.4302 - accuracy: 0.0759\n",
      "Epoch 42/50\n",
      "93/93 [==============================] - 17s 178ms/step - loss: 0.4143 - accuracy: 0.0781\n",
      "Epoch 43/50\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 0.3983 - accuracy: 0.0806\n",
      "Epoch 44/50\n",
      "93/93 [==============================] - 17s 177ms/step - loss: 0.3813 - accuracy: 0.0833\n",
      "Epoch 45/50\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 0.3656 - accuracy: 0.0858\n",
      "Epoch 46/50\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 0.3494 - accuracy: 0.0882\n",
      "Epoch 47/50\n",
      "93/93 [==============================] - 17s 177ms/step - loss: 0.3332 - accuracy: 0.0909\n",
      "Epoch 48/50\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 0.3164 - accuracy: 0.0936\n",
      "Epoch 49/50\n",
      "93/93 [==============================] - 17s 178ms/step - loss: 0.3003 - accuracy: 0.0963\n",
      "Epoch 50/50\n",
      "93/93 [==============================] - 17s 178ms/step - loss: 0.2843 - accuracy: 0.0989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7939840d44c0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "model1.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "model1.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97e71a25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "93/93 [==============================] - 51s 311ms/step - loss: 1.0343 - accuracy: 0.0200\n",
      "Epoch 2/50\n",
      "93/93 [==============================] - 29s 313ms/step - loss: 0.8948 - accuracy: 0.0204\n",
      "Epoch 3/50\n",
      "93/93 [==============================] - 29s 310ms/step - loss: 0.8799 - accuracy: 0.0232\n",
      "Epoch 4/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.8333 - accuracy: 0.0391\n",
      "Epoch 5/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.8014 - accuracy: 0.0396\n",
      "Epoch 6/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.7659 - accuracy: 0.0407\n",
      "Epoch 7/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.7426 - accuracy: 0.0420\n",
      "Epoch 8/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.7244 - accuracy: 0.0430\n",
      "Epoch 9/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.7085 - accuracy: 0.0436\n",
      "Epoch 10/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.6932 - accuracy: 0.0444\n",
      "Epoch 11/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.6768 - accuracy: 0.0453\n",
      "Epoch 12/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.6595 - accuracy: 0.0467\n",
      "Epoch 13/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.6415 - accuracy: 0.0479\n",
      "Epoch 14/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.6222 - accuracy: 0.0495\n",
      "Epoch 15/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.6031 - accuracy: 0.0509\n",
      "Epoch 16/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.5835 - accuracy: 0.0524\n",
      "Epoch 17/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.5635 - accuracy: 0.0542\n",
      "Epoch 18/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.5426 - accuracy: 0.0561\n",
      "Epoch 19/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.5228 - accuracy: 0.0581\n",
      "Epoch 20/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.5025 - accuracy: 0.0604\n",
      "Epoch 21/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.4824 - accuracy: 0.0625\n",
      "Epoch 22/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.4597 - accuracy: 0.0653\n",
      "Epoch 23/50\n",
      "93/93 [==============================] - 29s 311ms/step - loss: 0.4400 - accuracy: 0.0676\n",
      "Epoch 24/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.4185 - accuracy: 0.0704\n",
      "Epoch 25/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.3969 - accuracy: 0.0736\n",
      "Epoch 26/50\n",
      "93/93 [==============================] - 29s 311ms/step - loss: 0.3767 - accuracy: 0.0767\n",
      "Epoch 27/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.3558 - accuracy: 0.0801\n",
      "Epoch 28/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.3347 - accuracy: 0.0834\n",
      "Epoch 29/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.3142 - accuracy: 0.0869\n",
      "Epoch 30/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.2938 - accuracy: 0.0907\n",
      "Epoch 31/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.2742 - accuracy: 0.0942\n",
      "Epoch 32/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.2540 - accuracy: 0.0979\n",
      "Epoch 33/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.2352 - accuracy: 0.1016\n",
      "Epoch 34/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.2151 - accuracy: 0.1053\n",
      "Epoch 35/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.1975 - accuracy: 0.1088\n",
      "Epoch 36/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.1802 - accuracy: 0.1120\n",
      "Epoch 37/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.1627 - accuracy: 0.1153\n",
      "Epoch 38/50\n",
      "93/93 [==============================] - 29s 313ms/step - loss: 0.1467 - accuracy: 0.1183\n",
      "Epoch 39/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.1323 - accuracy: 0.1209\n",
      "Epoch 40/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.1175 - accuracy: 0.1236\n",
      "Epoch 41/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.1042 - accuracy: 0.1258\n",
      "Epoch 42/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.0919 - accuracy: 0.1279\n",
      "Epoch 43/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.0808 - accuracy: 0.1297\n",
      "Epoch 44/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.0714 - accuracy: 0.1311\n",
      "Epoch 45/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.0627 - accuracy: 0.1323\n",
      "Epoch 46/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.0542 - accuracy: 0.1335\n",
      "Epoch 47/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.0471 - accuracy: 0.1345\n",
      "Epoch 48/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.0412 - accuracy: 0.1352\n",
      "Epoch 49/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.0359 - accuracy: 0.1358\n",
      "Epoch 50/50\n",
      "93/93 [==============================] - 29s 312ms/step - loss: 0.0300 - accuracy: 0.1365\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x79390b5b8a90>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "model2.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "model2.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e65154c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "93/93 [==============================] - 101s 582ms/step - loss: 0.9699 - accuracy: 0.0201\n",
      "Epoch 2/50\n",
      "93/93 [==============================] - 54s 579ms/step - loss: 0.8837 - accuracy: 0.0204\n",
      "Epoch 3/50\n",
      "93/93 [==============================] - 54s 580ms/step - loss: 0.8825 - accuracy: 0.0202\n",
      "Epoch 4/50\n",
      "93/93 [==============================] - 54s 580ms/step - loss: 0.8819 - accuracy: 0.0203\n",
      "Epoch 5/50\n",
      "93/93 [==============================] - 54s 580ms/step - loss: 0.8816 - accuracy: 0.0203\n",
      "Epoch 6/50\n",
      "93/93 [==============================] - 54s 579ms/step - loss: 0.8814 - accuracy: 0.0203\n",
      "Epoch 7/50\n",
      "93/93 [==============================] - 54s 579ms/step - loss: 0.8811 - accuracy: 0.0204\n",
      "Epoch 8/50\n",
      "93/93 [==============================] - 54s 578ms/step - loss: 0.8809 - accuracy: 0.0206\n",
      "Epoch 9/50\n",
      "93/93 [==============================] - 54s 578ms/step - loss: 0.8783 - accuracy: 0.0206\n",
      "Epoch 10/50\n",
      "93/93 [==============================] - 54s 579ms/step - loss: 0.8717 - accuracy: 0.0215\n",
      "Epoch 11/50\n",
      "93/93 [==============================] - 54s 579ms/step - loss: 0.8606 - accuracy: 0.0221\n",
      "Epoch 12/50\n",
      "93/93 [==============================] - 54s 580ms/step - loss: 0.8522 - accuracy: 0.0225\n",
      "Epoch 13/50\n",
      "93/93 [==============================] - 54s 579ms/step - loss: 0.8468 - accuracy: 0.0225\n",
      "Epoch 14/50\n",
      "93/93 [==============================] - 54s 579ms/step - loss: 0.8439 - accuracy: 0.0226\n",
      "Epoch 15/50\n",
      "93/93 [==============================] - 54s 579ms/step - loss: 0.8416 - accuracy: 0.0229\n",
      "Epoch 16/50\n",
      "93/93 [==============================] - 54s 579ms/step - loss: 0.8394 - accuracy: 0.0229\n",
      "Epoch 17/50\n",
      "93/93 [==============================] - 54s 579ms/step - loss: 0.8382 - accuracy: 0.0228\n",
      "Epoch 18/50\n",
      "93/93 [==============================] - 54s 580ms/step - loss: 0.8369 - accuracy: 0.0231\n",
      "Epoch 19/50\n",
      "93/93 [==============================] - 54s 580ms/step - loss: 0.8348 - accuracy: 0.0230\n",
      "Epoch 20/50\n",
      "93/93 [==============================] - 54s 579ms/step - loss: 0.8339 - accuracy: 0.0230\n",
      "Epoch 21/50\n",
      "93/93 [==============================] - 54s 579ms/step - loss: 0.8322 - accuracy: 0.0230\n",
      "Epoch 22/50\n",
      "93/93 [==============================] - 54s 579ms/step - loss: 0.8316 - accuracy: 0.0230\n",
      "Epoch 23/50\n",
      "93/93 [==============================] - 54s 580ms/step - loss: 0.8299 - accuracy: 0.0232\n",
      "Epoch 24/50\n",
      "51/93 [===============>..............] - ETA: 24s - loss: 0.8302 - accuracy: 0.0232"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1623/3999585194.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "model3.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "model3.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81148fb2",
   "metadata": {},
   "source": [
    "## Step5. 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b6cefc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def decoder_inference(sentence):\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
    "  # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
    "  sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "  # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
    "  # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
    "  output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  # 디코더의 인퍼런스 단계\n",
    "  for i in range(MAX_LENGTH):\n",
    "    # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
    "    predictions = model3(inputs=[sentence, output_sequence], training=False)\n",
    "    predictions = predictions[:, -1:, :]\n",
    "\n",
    "    # 현재 예측한 단어의 정수\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
    "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "      break\n",
    "\n",
    "    # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
    "    # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
    "    output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output_sequence, axis=0)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3b59adec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def sentence_generation(sentence):\n",
    "    \n",
    "  # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
    "  prediction = decoder_inference(sentence)\n",
    "\n",
    "  # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
    "  predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "  print('입력 : {}'.format(sentence))\n",
    "  print('출력 : {}'.format(predicted_sentence))\n",
    "\n",
    "  return predicted_sentence\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "1a34e57c",
   "metadata": {},
   "source": [
    "### 다른 모델에 대해 동일한 쿼리 실행\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef054e2f",
   "metadata": {},
   "source": [
    "- model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c3ebe8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 12시 땡!\n",
      "출력 : 직장 스트레스 심하겠네요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'직장 스트레스 심하겠네요 .'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('12시 땡!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ceba023d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 1지망 학교가 떨어졌어\n",
      "출력 : 이제 좀 괜찮아졌길 바랍니다 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'이제 좀 괜찮아졌길 바랍니다 .'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('1지망 학교가 떨어졌어')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ad72eff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 3박 4일 놀러가고 싶다\n",
      "출력 : 뭘 입어도 만큼 좋은 일이 생길 거예요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'뭘 입어도 만큼 좋은 일이 생길 거예요 .'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('3박 4일 놀러가고 싶다')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395b4bbb",
   "metadata": {},
   "source": [
    "- model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b2707374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 12시 땡!\n",
      "출력 : 하루가 또 가네요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'하루가 또 가네요 .'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('12시 땡!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "186b802c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 1지망 학교가 떨어졌어\n",
      "출력 : 위로해 드립니다 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'위로해 드립니다 .'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('1지망 학교가 떨어졌어')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e0777e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 3박 4일 놀러가고 싶다\n",
      "출력 : 가려 먹으면 좋죠 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'가려 먹으면 좋죠 .'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('3박 4일 놀러가고 싶다')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bf7b5f",
   "metadata": {},
   "source": [
    "- model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0f12a381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 12시 땡!\n",
      "출력 : 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 '"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('12시 땡!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c86489e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 1지망 학교가 떨어졌어\n",
      "출력 : 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 '"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('1지망 학교가 떨어졌어')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2dfa725e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 3박 4일 놀러가고 싶다\n",
      "출력 : 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 저도 '"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('3박 4일 놀러가고 싶다')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf1a041",
   "metadata": {},
   "source": [
    "# 회고\n",
    "\n",
    "- 학습이 잘 일어나지 않았을때, 최대 길이만큼 텍스트를 반복한다는 것을 알게 됨. -> 최대 길이를 예상 대답 만큼 하는게 적합 한 것 같음.\n",
    "- 트랜스포머 모델의 깊이를 늘린다고 해서 항상 모델이 좋아지는건 아님.\n",
    "- 모델의 원리를 완벽히 파악하지 못해 하이퍼 파라미터 튜닝에 중점을 두고 진행함. 조금 더 보충 학습을 해서 모델에 대해 공부하고자 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5340f2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
